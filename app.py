import streamlit as st
import time
from backend import LocalLLM

# Page Setup
st.set_page_config(
    page_title="Local Qwen 2.5 7B Assistant",
    page_icon="ÔøΩ",
    layout="wide"
)

st.title("ÔøΩ Local Qwen 2.5 7B")
st.markdown("*Powered by Qwen-2.5-7B and llama.cpp*")

# Initialize Backend with Caching
# @st.cache_resource ensures the model is loaded only once
@st.cache_resource
def get_llm():
    return LocalLLM()

try:
    llm = get_llm()
except Exception as e:
    st.error(f"Error loading model: {e}")
    st.stop()

# Initialize Chat History
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display Chat History
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# User Input
if prompt := st.chat_input("Ask me a complex question..."):
    # Add user message to history
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generate Response
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            try:
                # generate_response now returns a tuple (text, context_str)
                start_time = time.time()
                response_text, context_str = llm.generate_response(st.session_state.messages)
                end_time = time.time()
                
                st.markdown(response_text)
                st.caption(f"‚è±Ô∏è Finished in {end_time - start_time:.2f}s")
                
                # Append assistant response to history
                st.session_state.messages.append({"role": "assistant", "content": response_text})
                
                # Update session state for sidebar context
                st.session_state.last_context = context_str
                
            except Exception as e:
                st.error(f"An error occurred: {e}")

# Sidebar for controls or info
with st.sidebar:
    st.header("About")
    st.info("This assistant uses a quantized Qwen 2.5 7B Instruct model running locally.")
    
    with st.expander("üìù H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng K√Ω ·ª©c"):
        st.markdown("""
        **1. Ghi nh·ªõ:**
        - `H√£y nh·ªõ: [n·ªôi dung]`
        - `Remember: [content]`
        *(Ho·∫∑c c·ª© n√≥i "T√¥i t√™n l√†..." m√°y s·∫Ω t·ª± nh·ªõ)*
        
        **2. Qu√™n:**
        - `Qu√™n: [n·ªôi dung c≈©]`
        - `Forget: [old content]`
        
        **3. C·∫≠p nh·∫≠t:**
        - `C·∫≠p nh·∫≠t: [n·ªôi dung m·ªõi]`
        - `Thay ƒë·ªïi: [n·ªôi dung m·ªõi]`
        *(M√°y s·∫Ω t·ª± t√¨m c√°i c≈© li√™n quan ƒë·ªÉ x√≥a v√† l∆∞u c√°i m·ªõi)*
        """)
    if st.button("Clear Chat"):
        st.session_state.messages = []
        st.session_state.last_context = ""
        st.rerun()

    st.divider()
    st.subheader("üß† Memory Debug of Last Reply")
    if "last_context" in st.session_state and st.session_state.last_context:
        st.info("Found relevant memories:")
        st.text(st.session_state.last_context)
    else:
        st.caption("No relevant memory found or not a query.")
